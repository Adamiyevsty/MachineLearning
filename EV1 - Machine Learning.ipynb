{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dace1d4",
   "metadata": {},
   "source": [
    "# Fase 1 - Business Understanding\n",
    "\n",
    "El banco Monopoly, con una larga trayectoria en el mercado chileno, ha sido recientemente adquirido por la entidad financiera internacional Dormammu. Ante este cambio, Dormammu ha encomendado a su equipo de ingenieros de datos la tarea de analizar exhaustivamente la base de clientes de Monopoly. El objetivo principal es comprender los hábitos financieros de esta nueva clientela y diseñar estrategias personalizadas que permitan una integración exitosa al ecosistema de Dormammu. Para ello, se ha puesto a disposición de los ingenieros una base de datos que abarca un año de información transaccional de una muestra representativa de clientes de Monopoly. La labor del equipo consiste en limpiar y analizar estos datos, identificando patrones de consumo, preferencias y necesidades financieras. Con esta información, se busca generar insights valiosos que permitan a Dormammu diseñar una oferta de productos y servicios a medida, optimizando así la experiencia del cliente y maximizando la rentabilidad.\n",
    "\n",
    "Para esto, como equipo analista de datos vamos a darle respuesta a las siguientes incognitas:\n",
    "\n",
    "- ¿Cuáles son los 3 meses de mayor uso de tarjetas de crédito a nivel nacional e internacional?\n",
    "- ¿Existen diferentes grupos de clientes basados en su comportamiento financiero, como el uso de tarjetas de crédito, nivel de deuda y productos contratados?\n",
    "- ¿Cuál es la probabilidad de que los clientes con ciertas características demográficas o comportamiento histórico caigan en mora?\n",
    "- ¿Los clientes con mayor antigüedad o que usan múltiples productos son más fieles al banco?\n",
    "- ¿Los clientes con dualidad (dos o más tarjetas de crédito) son más propensos a utilizar servicios adicionales como avances en cuotas o compras internacionales?\n",
    "\n",
    "Con lo expuesto anteriormente, se espera responder satisfactoriamente a estas preguntas para que el nuevo dueño del banco pueda tener información sólida y conocer a mayor profundidad las interacciones económicas de sus clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2c24a",
   "metadata": {},
   "source": [
    "# Fase 2 - Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6fd61",
   "metadata": {},
   "source": [
    "### A continuación se importarán las librerías necesarias para trabajar en esta metodología.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af846be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerias\n",
    "\n",
    "%pip install pyarrow\n",
    "%pip install tpot\n",
    "%pip install --upgrade setuptools\n",
    "%pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
    "%pip install ipywidgets\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy import stats \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a43c76",
   "metadata": {},
   "source": [
    "# AUTOMATIZACIÓN *\n",
    "### Se realiza conversión del archivo \"Base_clientes_Monopoly.xlsx\", al formato de archivo '.parquet' en caso de no existir en las rutas predefinidas, para una mayor eficiencia en la carga y análisis de los datos. MODIFICAR la ruta de salida para el archivo '.parquet' si este no existe en el sistema en el que se ejecuta este Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las posibles rutas de salida para el archivo Parquet\n",
    "ruta_parquet_1 = \"/Users/herna/Desktop/Duoc_UC/6to SEMESTRE/Machine_Learning/ET/Base_clientes_Monopoly.parquet\"\n",
    "ruta_parquet_2 = \"/Users/new11/Documents/DUOC - Ing. Informática/2024/2024-2/Machine Learning/Base_clientes_Monopoly.parquet\"\n",
    "ruta_parquet_3 = \"/Users/abrahamrubilaralbarran/Desktop/DataSet Monopoly /Base_clientes_Monopoly.parquet\"\n",
    "\n",
    "# Verificar si el archivo Parquet ya existe en alguna de las rutas de salida\n",
    "if os.path.exists(ruta_parquet_1):\n",
    "    ruta_parquet = ruta_parquet_1\n",
    "    print(f\"Archivo .parquet encontrado en {ruta_parquet}. Cargando directamente.\")\n",
    "elif os.path.exists(ruta_parquet_2):\n",
    "    ruta_parquet = ruta_parquet_2\n",
    "    print(f\"Archivo .parquet encontrado en {ruta_parquet}. Cargando directamente.\")\n",
    "elif os.path.exists(ruta_parquet_3):\n",
    "    ruta_parquet = ruta_parquet_3\n",
    "    print(f\"Archivo .parquet encontrado en {ruta_parquet}. Cargando directamente.\")\n",
    "else:\n",
    "    # Si no existe el archivo .parquet, procede con la carga y conversión desde el archivo Excel\n",
    "    print(\"'Base_clientes_Monopoly.parquet' no encontrado. Procediendo a cargar y convertir desde el 'Base_clientes_Monopoly.xlsx'.\")\n",
    "    \n",
    "    # Definir las rutas de entrada del archivo Excel\n",
    "    ruta_excel_1 = \"/Users/herna/Desktop/Duoc_UC/6to SEMESTRE/Machine_Learning/ET/Base_clientes_Monopoly.xlsx\"\n",
    "    ruta_excel_2 = \"/Users/new11/Documents/DUOC - Ing. Informática/2024/2024-2/Machine Learning/Dataset Monopoly.xlsx\"\n",
    "    ruta_excel_3 = \"/Users/abrahamrubilaralbarran/Desktop/DataSet Monopoly /Base_clientes_Monopoly.xlsx\"\n",
    "    \n",
    "    # Verificar cuál de las rutas del archivo Excel existe para cargar el archivo\n",
    "    if os.path.exists(ruta_excel_1):\n",
    "        ruta_excel = ruta_excel_1\n",
    "        print(f\"Cargando archivo Excel desde {ruta_excel}\")\n",
    "    elif os.path.exists(ruta_excel_2):\n",
    "        ruta_excel = ruta_excel_2\n",
    "        print(f\"Cargando archivo Excel desde {ruta_excel}\")\n",
    "    elif os.path.exists(ruta_excel_3):\n",
    "        ruta_excel = ruta_excel_3\n",
    "        print(f\"Cargando archivo Excel desde {ruta_excel}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Ninguna de las rutas de archivo Excel existe.\")\n",
    "    \n",
    "    # Cargar el archivo Excel\n",
    "    df = pd.read_excel(ruta_excel, engine=\"openpyxl\")\n",
    "    df.columns = df.iloc[0]  # Configurar la primera fila como encabezado\n",
    "    df1 = df.iloc[1:].reset_index(drop=True)  # Eliminar la primera fila de encabezados redundantes\n",
    "    \n",
    "    # Asignar la ruta de salida predeterminada para el archivo Parquet\n",
    "    ruta_parquet = ruta_parquet_1\n",
    "    \n",
    "    # Convertir y guardar en formato parquet\n",
    "    df1.to_parquet(ruta_parquet, engine=\"pyarrow\")\n",
    "    print(f\"Archivo convertido y guardado en {ruta_parquet}\")\n",
    "\n",
    "# Cargar el archivo Parquet (ya sea existente o recién creado) para verificar su contenido\n",
    "df = pd.read_parquet(ruta_parquet, engine=\"pyarrow\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf9823",
   "metadata": {},
   "source": [
    "### Como podemos ver en el head() anterior, tenemos la última columna repleta de valores NaN, por lo que vamos a borrar esa columna por completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb6b76",
   "metadata": {},
   "source": [
    "### Para poder proceder al análisis exploratorio del Dataframe, seleccionaremos las columnas más relevantes para encontrar respuesta a las preguntas expuestas en la primera fase.\n",
    "### Para esto, crearemos un Array el cual va a contener todos los nombres de las columnas que se utilizarán para el análisis, se establece un nuevo Dataframe con las columnas seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0061c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el Array y se insertan el nombre de las columnas seleccionadas\n",
    "columnas_permitidas = ['Id', 'Edad', 'Renta', 'Region', 'Sexo', 'TC', 'Cuentas', 'Hipotecario', 'Consumo', 'Debito', \n",
    "              'Ctacte', 'Antiguedad', 'Dualidad','FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', \n",
    "              'FacCN_T05', 'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10', 'FacCN_T11', \n",
    "              'FacCN_T12', 'FacCI_T01', 'FacCI_T02', 'FacCI_T03', 'FacCI_T04', 'FacCI_T05', 'FacCI_T06', \n",
    "              'FacCI_T07', 'FacCI_T08', 'FacCI_T09', 'FacCI_T10', 'FacCI_T11', 'FacCI_T12', 'TxsCN_T01', \n",
    "              'TxsCN_T02', 'TxsCN_T03', 'TxsCN_T04', 'TxsCN_T05', 'TxsCN_T06', 'TxsCN_T07', 'TxsCN_T08', \n",
    "              'TxsCN_T09', 'TxsCN_T10', 'TxsCN_T11', 'TxsCN_T12', 'TxsCI_T01', 'TxsCI_T02', 'TxsCI_T03', \n",
    "              'TxsCI_T04', 'TxsCI_T05', 'TxsCI_T06', 'TxsCI_T07', 'TxsCI_T08', 'TxsCI_T09', 'TxsCI_T10', \n",
    "              'TxsCI_T11', 'TxsCI_T12', 'UsoL1_T01','UsoL1_T02', 'UsoL1_T03', 'UsoL1_T04', 'UsoL1_T05', \n",
    "              'UsoL1_T06', 'UsoL1_T07', 'UsoL1_T08', 'UsoL1_T09', 'UsoL1_T10', 'UsoL1_T11', 'UsoL1_T12', \n",
    "              'UsoLI_T01', 'UsoLI_T02', 'UsoLI_T03', 'UsoLI_T04', 'UsoLI_T05', 'UsoLI_T06', 'UsoLI_T07', \n",
    "              'UsoLI_T08', 'UsoLI_T09', 'UsoLI_T10', 'UsoLI_T11', 'UsoLI_T12', 'CUPO_L1', 'CUPO_MX', \n",
    "              'PagoNac_T01', 'PagoNac_T02', 'PagoNac_T03', 'PagoNac_T04', 'PagoNac_T05', 'PagoNac_T06', \n",
    "              'PagoNac_T07', 'PagoNac_T08', 'PagoNac_T09', 'PagoNac_T10', 'PagoNac_T11', 'PagoNac_T12', \n",
    "              'PagoInt_T01', 'PagoInt_T02', 'PagoInt_T03', 'PagoInt_T04', 'PagoInt_T05', 'PagoInt_T06', \n",
    "              'PagoInt_T07', 'PagoInt_T08', 'PagoInt_T09', 'PagoInt_T10', 'PagoInt_T11', 'PagoInt_T12', \n",
    "              'FlgAct_T01', 'FlgAct_T02', 'FlgAct_T03', 'FlgAct_T04', 'FlgAct_T05', 'FlgAct_T06', 'FlgAct_T07', \n",
    "              'FlgAct_T08', 'FlgAct_T09', 'FlgAct_T10', 'FlgAct_T11', 'FlgAct_T12', 'FacAN_T01', 'FacAN_T02', \n",
    "              'FacAN_T03', 'FacAN_T04', 'FacAN_T05', 'FacAN_T06', 'FacAN_T07', 'FacAN_T08', 'FacAN_T09', \n",
    "              'FacAN_T10', 'FacAN_T11', 'FacAN_T12', 'FacAI_T01', 'FacAI_T02', 'FacAI_T03', 'FacAI_T04', \n",
    "              'FacAI_T05', 'FacAI_T06', 'FacAI_T07', 'FacAI_T08', 'FacAI_T09', 'FacAI_T10', 'FacAI_T11', \n",
    "              'FacAI_T12', 'target','IndRev_T12','IndRev_T11','IndRev_T10','IndRev_T09','IndRev_T08',\n",
    "              'IndRev_T07','IndRev_T06','IndRev_T05','IndRev_T04','IndRev_T03','IndRev_T02','IndRev_T01']\n",
    "\n",
    "# Se establece el nuevo nombre del Dataframe que contiene las columnas seleccionadas\n",
    "df_acotado = df[columnas_permitidas]\n",
    "\n",
    "# Iniciamos con el primer paso de la exploración de los datos\n",
    "df_acotado.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46df89",
   "metadata": {},
   "source": [
    "### En la celda ejecutada anteriormente, se aprecian todos los tipos de datos del Dataframe. Ahora revisamos cuáles son los valores únicos dentro de estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_acotado:\n",
    "    print(f\"{i} = {df_acotado[i].unique()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167bcd6",
   "metadata": {},
   "source": [
    "### En el resultado de la celda anterior, nos pudimos percatar que al mostrar los valores únicos, hay filas que no tienen valor asignado (NaN), por lo que ahora vamos a sumar todas las filas que tienen al menos una entrada de este tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acotado.isna().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa2c18",
   "metadata": {},
   "source": [
    "# ⬆⬆⬆\n",
    "### Esto nos indica que hay 19.862 filas que tienen al menos un valor en alguna de las columnas como NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daca172",
   "metadata": {},
   "source": [
    "### La siguiente celda nos permite revisar cómo están compuestos los datos de cada columna, nos entrega información relevante como el promedio, valor mínimo, máximo, cuartiles y el tipo de dato. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eba739",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_acotado:\n",
    "  print(f\"{i} = {df_acotado[i].describe()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18087629",
   "metadata": {},
   "source": [
    "### A continuación vamos a ramificar el Dataframe original para poder hacer un estudio con gráficos, ya que, se deben separar los valores numéricos y categoricos. Para esto vamos a hacer un Array para contener las columnas categóricas encontradas en el Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c511f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del Array\n",
    "columnas_a_seleccionar = []\n",
    "\n",
    "# Ciclo for para iterar sobre los nombres de las columnas que empiezan con \"IndRev_\"\n",
    "for columna in df_acotado.columns:\n",
    "    if columna.startswith('IndRev_'):\n",
    "        columnas_a_seleccionar.append(columna)\n",
    "\n",
    "# Seleccionar las columnas \"IndRev_\" en un nuevo Dataframe\n",
    "df_categorico = df_acotado[columnas_a_seleccionar]\n",
    "\n",
    "# Creamos un segundo Dataframe con las siguientes columnas\n",
    "df_categorico2 = df_acotado[['Sexo', 'target']]\n",
    "\n",
    "# Y concatenamos los Dataframe en unos solo\n",
    "df_concatenado_categorico = pd.concat([df_categorico2, df_categorico], axis=1)\n",
    "df_concatenado_categorico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6d996",
   "metadata": {},
   "source": [
    "### Sustraemos todas las columnas categóricas del Dataframe original para renombrarlo como un nuevo Dataframe de sólo columnas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acotado_numericas = df_acotado.drop(['Sexo','target','IndRev_T12','IndRev_T11','IndRev_T10','IndRev_T09','IndRev_T08','IndRev_T07','IndRev_T06','IndRev_T05','IndRev_T04','IndRev_T03','IndRev_T02','IndRev_T01'], axis=1)\n",
    "df_acotado_numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cd39b",
   "metadata": {},
   "source": [
    "### Revisamos el data frame de las variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_acotado_numericas:\n",
    "  print(f\"{i} = {df_acotado_numericas[i].describe()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f61f80",
   "metadata": {},
   "source": [
    "### Revisamos las correlaciones que tienen las variables numéricas en nuestro nuevo Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2933e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_correlations(dataframe, top_n=30):\n",
    "    # Calcular la matriz de correlación\n",
    "    corr_matrix = dataframe.corr().abs()\n",
    "    \n",
    "    # Extraer las correlaciones superiores a la diagonal\n",
    "    triangulo_matriz = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Encontrar las correlaciones más altas\n",
    "    top_corr_pairs = (\n",
    "        triangulo_matriz.unstack()\n",
    "        .dropna()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    \n",
    "    # Obtener las columnas de las correlaciones más altas\n",
    "    top_columnas = list(set([index[0] for index in top_corr_pairs.index] + [index[1] for index in top_corr_pairs.index]))\n",
    "    \n",
    "    # Crear un nuevo dataframe con solo las columnas seleccionadas\n",
    "    top_corr_dataframe = dataframe[top_columnas]\n",
    "    \n",
    "    # Calcular la matriz de correlación del nuevo dataframe\n",
    "    top_corr_matriz = top_corr_dataframe.corr()\n",
    "    \n",
    "    # Dibujar el mapa de calor\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sb.heatmap(top_corr_matriz, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
    "    plt.title('Top {} Feature Correlations'.format(top_n))\n",
    "    plt.show()\n",
    "\n",
    "# Uso de la función\n",
    "# Asegúrate de reemplazar 'your_dataframe' con el nombre de tu dataframe\n",
    "plot_top_correlations(df_acotado_numericas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89386d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las columnas que queremos analizar en detalle\n",
    "columnas_para_filtrar = ['Edad', 'Region', 'Antiguedad']\n",
    "\n",
    "# Iteramos sobre cada columna del DataFrame, excluyendo un gran conjunto de columnas\n",
    "for columna in df_acotado.drop(df_acotado[['Id','FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', \n",
    "              'FacCN_T05', 'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10', 'FacCN_T11', \n",
    "              'FacCN_T12', 'FacCI_T01', 'FacCI_T02', 'FacCI_T03', 'FacCI_T04', 'FacCI_T05', 'FacCI_T06', \n",
    "              'FacCI_T07', 'FacCI_T08', 'FacCI_T09', 'FacCI_T10', 'FacCI_T11', 'FacCI_T12', 'TxsCN_T01', \n",
    "              'TxsCN_T02', 'TxsCN_T03', 'TxsCN_T04', 'TxsCN_T05', 'TxsCN_T06', 'TxsCN_T07', 'TxsCN_T08', \n",
    "              'TxsCN_T09', 'TxsCN_T10', 'TxsCN_T11', 'TxsCN_T12', 'TxsCI_T01', 'TxsCI_T02', 'TxsCI_T03', \n",
    "              'TxsCI_T04', 'TxsCI_T05', 'TxsCI_T06', 'TxsCI_T07', 'TxsCI_T08', 'TxsCI_T09', 'TxsCI_T10', \n",
    "              'TxsCI_T11', 'TxsCI_T12', 'UsoL1_T01','UsoL1_T02', 'UsoL1_T03', 'UsoL1_T04', 'UsoL1_T05', \n",
    "              'UsoL1_T06', 'UsoL1_T07', 'UsoL1_T08', 'UsoL1_T09', 'UsoL1_T10', 'UsoL1_T11', 'UsoL1_T12', \n",
    "              'UsoLI_T01', 'UsoLI_T02', 'UsoLI_T03', 'UsoLI_T04', 'UsoLI_T05', 'UsoLI_T06', 'UsoLI_T07', \n",
    "              'UsoLI_T08', 'UsoLI_T09', 'UsoLI_T10', 'UsoLI_T11', 'UsoLI_T12', \n",
    "              'PagoNac_T01', 'PagoNac_T02', 'PagoNac_T03', 'PagoNac_T04', 'PagoNac_T05', 'PagoNac_T06', \n",
    "              'PagoNac_T07', 'PagoNac_T08', 'PagoNac_T09', 'PagoNac_T10', 'PagoNac_T11', 'PagoNac_T12', \n",
    "              'PagoInt_T01', 'PagoInt_T02', 'PagoInt_T03', 'PagoInt_T04', 'PagoInt_T05', 'PagoInt_T06', \n",
    "              'PagoInt_T07', 'PagoInt_T08', 'PagoInt_T09', 'PagoInt_T10', 'PagoInt_T11', 'PagoInt_T12', \n",
    "              'FlgAct_T01', 'FlgAct_T02', 'FlgAct_T03', 'FlgAct_T04', 'FlgAct_T05', 'FlgAct_T06', 'FlgAct_T07', \n",
    "              'FlgAct_T08', 'FlgAct_T09', 'FlgAct_T10', 'FlgAct_T11', 'FlgAct_T12', 'FacAN_T01', 'FacAN_T02', \n",
    "              'FacAN_T03', 'FacAN_T04', 'FacAN_T05', 'FacAN_T06', 'FacAN_T07', 'FacAN_T08', 'FacAN_T09', \n",
    "              'FacAN_T10', 'FacAN_T11', 'FacAN_T12', 'FacAI_T01', 'FacAI_T02', 'FacAI_T03', 'FacAI_T04', \n",
    "              'FacAI_T05', 'FacAI_T06', 'FacAI_T07', 'FacAI_T08', 'FacAI_T09', 'FacAI_T10', 'FacAI_T11', \n",
    "              'FacAI_T12', 'IndRev_T12','IndRev_T11','IndRev_T10','IndRev_T09','IndRev_T08',\n",
    "              'IndRev_T07','IndRev_T06','IndRev_T05','IndRev_T04','IndRev_T03','IndRev_T02','IndRev_T01']], axis=1):\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    sb.histplot(df_acotado[columna], bins=200)\n",
    "    plt.title(columna)\n",
    "    plt.xlabel(columna)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "    # Si la columna está en la lista de columnas a filtrar, ajustamos los límites de los ejes x\n",
    "    if columna in columnas_para_filtrar:\n",
    "        valor_max = df_acotado[columna].max()\n",
    "        valor_min = df_acotado[columna].min()\n",
    "        plt.xticks(np.arange(valor_min, valor_max + 1, step=1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfea71",
   "metadata": {},
   "source": [
    "### Una vez visto los gráficos, ahora veremos la distribución de los datos, destacando los cuartiles, los valores extremos (outliers) y algunas estadísticas básicas que podremos ver de forma gráfica de tres características posiblemente importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a analizar\n",
    "columnas_a_analizar = ['Edad', 'Region', 'Antiguedad']\n",
    "\n",
    "# Definir subconjuntos de edad en más rangos\n",
    "df_sub_edad_menor_20 = df_acotado[df_acotado['Edad'] < 20]\n",
    "df_sub_edad_joven = df_acotado[(df_acotado['Edad'] >= 20) & (df_acotado['Edad'] < 30)]\n",
    "df_sub_edad_adulto = df_acotado[(df_acotado['Edad'] >= 30) & (df_acotado['Edad'] <= 60)]\n",
    "df_sub_edad_mayor = df_acotado[(df_acotado['Edad'] > 60) & (df_acotado['Edad'] <= 70)]\n",
    "df_sub_edad_mayor_70 = df_acotado[df_acotado['Edad'] > 70]\n",
    "\n",
    "# Transparencia para los scatterplots\n",
    "alpha_value = 0.5\n",
    "\n",
    "# Contar el número de regiones únicas, incluyendo NaN\n",
    "num_regiones = df_acotado['Region'].nunique()\n",
    "\n",
    "# Mapa de colores personalizado para las regiones, incluyendo NaN\n",
    "colores_regiones = sb.color_palette(\"tab20\", num_regiones + 1)  # Incluye un color para NaN\n",
    "\n",
    "for columna in columnas_a_analizar:\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Cálculo de la media y mediana\n",
    "    media = df_acotado[columna].mean()\n",
    "    mediana = df_acotado[columna].median()\n",
    "\n",
    "    # Cálculo del rango intercuartílico (IQR)\n",
    "    Q1 = df_acotado[columna].quantile(0.25)\n",
    "    Q3 = df_acotado[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lim_inferior = Q1 - 1.5 * IQR\n",
    "    lim_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Boxplot con IQR destacado\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sb.boxplot(x=df_acotado[columna], palette='Set2')\n",
    "\n",
    "    # Resaltar el IQR en el boxplot\n",
    "    plt.axvspan(Q1, Q3, color='lightgreen', alpha=0.3, label=f'IQR: {IQR:.2f} (Q1: {Q1:.2f}, Q3: {Q3:.2f})')\n",
    "    \n",
    "    # Líneas para los límites de los outliers\n",
    "    plt.axvline(lim_inferior, color='red', linestyle='dashdot', label=f'Límite Inferior de Outliers: {lim_inferior:.2f}')\n",
    "    plt.axvline(lim_superior, color='red', linestyle='dashdot', label=f'Límite Superior de Outliers: {lim_superior:.2f}')\n",
    "    \n",
    "    # Líneas para la media y mediana\n",
    "    plt.axvline(media, color='blue', linestyle='-', label=f'Media: {media:.2f}', linewidth=2)\n",
    "    plt.axvline(mediana, color='purple', linestyle='-', label=f'Mediana: {mediana:.2f}', linewidth=2)\n",
    "\n",
    "    # Asegurarse de que los ticks en el eje X sean enteros (solo para la columna Región)\n",
    "    if columna == 'Region':\n",
    "        plt.xticks(np.arange(1, num_regiones + 1, step=1), [str(i) for i in range(1, num_regiones + 1)])\n",
    "    \n",
    "    plt.title(f'Boxplot de {columna}')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0)  # Leyenda fuera del gráfico\n",
    "    \n",
    "    # Scatterplot con transparencia y coloración por valores (Regiones ordenadas)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    if columna == 'Region':\n",
    "        # Ordenar las regiones y asignar colores\n",
    "        df_acotado['Region'] = df_acotado['Region'].fillna(0)  # Llenar NaN con 0 temporalmente\n",
    "        regiones_ordenadas = np.sort(df_acotado['Region'].unique())  # Ordenar las regiones numéricamente, NaN quedará en 0\n",
    "\n",
    "        # Separar la etiqueta para Región NaN\n",
    "        regiones_ordenadas = regiones_ordenadas[regiones_ordenadas != 0]  # Excluir temporalmente Región 0 (NaN)\n",
    "        regiones_ordenadas = np.append(regiones_ordenadas, 0)  # Añadir Región 0 (NaN) al final\n",
    "\n",
    "        for idx, region in enumerate(regiones_ordenadas):\n",
    "            if region == 0:\n",
    "                label = 'Región NaN'  # Etiqueta para NaN\n",
    "            else:\n",
    "                label = f'Región {int(region)}'  # Etiquetas para las regiones numéricas\n",
    "            \n",
    "            df_region = df_acotado[df_acotado['Region'] == region]\n",
    "            plt.scatter(df_region['Id'], df_region['Region'], alpha=alpha_value, label=label, c=[colores_regiones[idx]])\n",
    "\n",
    "        # Ajustar el eje Y para que muestre los números enteros del 1 al 13\n",
    "        plt.yticks(np.arange(1, 14, step=1), [str(i) for i in range(1, 14)])\n",
    "        plt.xticks(np.arange(0, df_acotado['Id'].max(), step=10000))  # Asegura que las regiones se muestren adecuadamente\n",
    "    else:\n",
    "        plt.scatter(df_acotado['Id'], df_acotado[columna], alpha=alpha_value, c=df_acotado[columna], cmap='viridis')\n",
    "\n",
    "    # Resaltar IQR en el scatterplot\n",
    "    plt.axhspan(Q1, Q3, color='lightgreen', alpha=0.3, label='Rango Intercuartílico (IQR)')\n",
    "    plt.axhline(lim_inferior, color='red', linestyle='--', label='Límite Inferior de Outliers')\n",
    "    plt.axhline(lim_superior, color='red', linestyle='--', label='Límite Superior de Outliers')\n",
    "\n",
    "    # Líneas para la media y mediana en el scatterplot\n",
    "    plt.axhline(media, color='blue', linestyle='-', label=f'Media: {media:.2f}', linewidth=2)\n",
    "    plt.axhline(mediana, color='purple', linestyle='-', label=f'Mediana: {mediana:.2f}', linewidth=2)\n",
    "    \n",
    "    plt.title(f'Scatterplot de {columna}')\n",
    "    plt.xlabel('Id')\n",
    "    plt.ylabel(columna)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0)  # Leyenda fuera del gráfico\n",
    "\n",
    "    # Añadir más ticks en el eje X para columnas específicas\n",
    "    if columna == 'Edad':\n",
    "        plt.xticks(np.arange(df_acotado['Edad'].min(), df_acotado['Edad'].max() + 1, step=5))\n",
    "    elif columna == 'Region':\n",
    "        plt.xticks(np.arange(1, num_regiones + 1, step=1))  # Mostrar regiones del 1 a num_regiones\n",
    "    elif columna == 'Antiguedad':\n",
    "        plt.xticks(np.arange(df_acotado['Antiguedad'].min(), df_acotado['Antiguedad'].max() + 1, step=2))\n",
    "\n",
    "    # Scatterplot con subconjuntos por grupos de edad, añadiendo nuevos rangos\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if columna == 'Edad':\n",
    "        plt.scatter(df_sub_edad_menor_20['Id'], df_sub_edad_menor_20[columna], alpha=alpha_value, label='Edad < 20', c='lightblue')\n",
    "        plt.scatter(df_sub_edad_joven['Id'], df_sub_edad_joven[columna], alpha=alpha_value, label='20 <= Edad < 30', c='blue')\n",
    "        plt.scatter(df_sub_edad_adulto['Id'], df_sub_edad_adulto[columna], alpha=alpha_value, label='30 <= Edad <= 60', c='green')\n",
    "        plt.scatter(df_sub_edad_mayor['Id'], df_sub_edad_mayor[columna], alpha=alpha_value, label='60 < Edad <= 70', c='orange')\n",
    "        plt.scatter(df_sub_edad_mayor_70['Id'], df_sub_edad_mayor_70[columna], alpha=alpha_value, label='Edad > 70', c='red')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0)  # Leyenda fuera del gráfico\n",
    "        plt.title(f'Scatterplot de {columna} por grupos de edad')\n",
    "\n",
    "    elif columna == 'Region':\n",
    "        # Scatterplot con valores NaN en negro\n",
    "        for region in regiones_ordenadas:\n",
    "            if region == 0:\n",
    "                df_region_nan = df_acotado[df_acotado['Region'] == 0]  # Filtrar Región NaN\n",
    "                plt.scatter(df_region_nan['Id'], df_region_nan['Region'], alpha=alpha_value, label='Región NaN', c='black')\n",
    "            else:\n",
    "                df_region = df_acotado[df_acotado['Region'] == region]\n",
    "                plt.scatter(df_region['Id'], df_region[columna], alpha=alpha_value, label=f'Región {int(region)}')\n",
    "\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0)  # Leyenda fuera del gráfico\n",
    "        plt.title(f'Scatterplot de {columna} por Región')\n",
    "\n",
    "    elif columna == 'Antiguedad':\n",
    "        # Subconjuntos de antigüedad\n",
    "        df_sub_antiguo = df_acotado[df_acotado['Antiguedad'] > 12]\n",
    "        df_sub_nuevo = df_acotado[df_acotado['Antiguedad'] <= 12]\n",
    "        plt.scatter(df_sub_antiguo['Id'], df_sub_antiguo[columna], alpha=alpha_value, label='Antigüedad > 12', c='orange')\n",
    "        plt.scatter(df_sub_nuevo['Id'], df_sub_nuevo[columna], alpha=alpha_value, label='Antigüedad <= 12', c='purple')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0)  # Leyenda fuera del gráfico\n",
    "        plt.title(f'Scatterplot de {columna} por Antigüedad')\n",
    "\n",
    "    # Ajustar diseño para evitar solapamientos\n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb4f10",
   "metadata": {},
   "source": [
    "### A continuación revisaremos los datos de la columna Región para saber exactamente cómo se distribuyen los clientes a lo largo del país."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0375b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = df_acotado['Region']\n",
    "region.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af2ef7",
   "metadata": {},
   "source": [
    "### Revisamos un poco de estadística básica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar los valores NaN en la columna 'Region'\n",
    "df_acotado['Region'].dropna(inplace=True)\n",
    "\n",
    "# Eliminar los valores 0 de la columna 'Region'\n",
    "df_acotado = df_acotado[df_acotado['Region'] != 0]\n",
    "\n",
    "# Mostrar las estadísticas descriptivas de la columna 'Region'\n",
    "print(df_acotado['Region'].describe())\n",
    "\n",
    "# Mostrar los valores únicos en la columna 'Region'\n",
    "print(df_acotado['Region'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f531085",
   "metadata": {},
   "source": [
    "### Verificamos que estamos tratando el número de la región como un dato entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = df_acotado['Region']\n",
    "region = region.astype(int)\n",
    "region.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c75745",
   "metadata": {},
   "source": [
    "### Y seleccionados los rangos que se mostrarán en el gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48100d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los gráficos de torta contarán el número datos entre los rangos y lo mostraremos en un gráfico.\n",
    "n1 = region.loc[region == 1].count()\n",
    "n2 = region.loc[region == 2].count()\n",
    "n3 = region.loc[region == 3].count()\n",
    "n4 = region.loc[region == 4].count()\n",
    "n5 = region.loc[region == 5].count()\n",
    "n6 = region.loc[region == 6].count()\n",
    "n7 = region.loc[region == 7].count()\n",
    "n8 = region.loc[region == 8].count()\n",
    "n9 = region.loc[region == 9].count()\n",
    "n10 = region.loc[region == 10].count()\n",
    "n11 = region.loc[region == 11].count()\n",
    "n12 = region.loc[region == 12].count()\n",
    "n13 = region.loc[region == 13].count()\n",
    "print(n1,n2,n3,n4,n5,n6,n7,n8,n9,n10,n11,n12,n13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formato del grafico circular\n",
    "datos = [n1,n2,n3,n4,n5,n6,n7,n8,n9,n10,n11,n12,n13]\n",
    "\n",
    "# Separación de cada trozo de la torta al centro \n",
    "exp = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "\n",
    "# Etiquetas de cada trozo\n",
    "m = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\"]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.title(\"Distribución del volumen de registros por región\")\n",
    "plt.pie(datos, labels = m, explode = exp, autopct='%2.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce3108",
   "metadata": {},
   "source": [
    "# Fase 3 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989b93e",
   "metadata": {},
   "source": [
    "En la fase de Preparación de Datos consiste en transformar los datos crudos en un formato apto para análisis y modelado. Esto implica limpiar los datos (corrigiendo errores, completando valores faltantes), transformarlos (normalizando, estandarizando), construir muestras representativas y crear nuevas variables si es necesario.\n",
    "\n",
    "Como pudimos apreciar anteriormente, todas las columnas poseen datos tipo object, por lo tanto, tenemos que transformar cada columna a su mejor tipo de dato. *IMPORTANTE* Por ahora vamos a trabajar solamente con algunas cuantas columnas que utilizaremos para hacer un tipo de testeo de imputación de datos, además de hacer un pequeño análisis antes de explorar todos los datos como conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame para modificar los tipos de datos\n",
    "df_nuevos_dtype = df_acotado.copy()\n",
    "\n",
    "# Columnas que deben ser enteros\n",
    "columnas_enteros = [\n",
    "    'Id', 'Edad', 'Region', 'TC', 'Cuentas', 'Hipotecario', \n",
    "    'Consumo', 'Debito', 'Ctacte', 'Antiguedad', 'Dualidad', 'target'\n",
    "]\n",
    "for col in columnas_enteros:\n",
    "    df_nuevos_dtype[col] = df_nuevos_dtype[col].astype('int64')\n",
    "\n",
    "# Convertir columnas de tipo float64\n",
    "columnas_flotantes = [col for col in df_nuevos_dtype.columns if col not in columnas_enteros + ['Sexo', 'IndRev_T12', 'IndRev_T11', 'IndRev_T10', 'IndRev_T09', 'IndRev_T08', 'IndRev_T07', 'IndRev_T06', 'IndRev_T05', 'IndRev_T04', 'IndRev_T03', 'IndRev_T02', 'IndRev_T01']]\n",
    "df_nuevos_dtype[columnas_flotantes] = df_nuevos_dtype[columnas_flotantes].astype('float64')\n",
    "\n",
    "# Convertir columnas de tipo object\n",
    "columnas_objeto = ['Sexo', 'IndRev_T12', 'IndRev_T11', 'IndRev_T10', 'IndRev_T09', 'IndRev_T08', 'IndRev_T07', 'IndRev_T06', 'IndRev_T05', 'IndRev_T04', 'IndRev_T03', 'IndRev_T02', 'IndRev_T01']\n",
    "df_nuevos_dtype[columnas_objeto] = df_nuevos_dtype[columnas_objeto].astype('object')\n",
    "\n",
    "# Verificar tipos de datos\n",
    "for i in df_nuevos_dtype:\n",
    "    print(f\"{i} = {df_nuevos_dtype[i].dtype} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f8279",
   "metadata": {},
   "source": [
    "### [MODIFICAR] Ahora vamos a eliminar todas las filas que contengan valor nulo en la columna \"Renta\" y en la columna \"Region\", ya que según contexto, todos los clientes ingresados en el Dataset original, tienen alguna relación con la tenencia de tarjeta de crédito, la cual tiene como por prerrequisito, tener una renta mínima en el sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92284ff5",
   "metadata": {},
   "source": [
    "### Tomaremos a la Región 13, ya que concentra la mayor cantidad de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_limpio = df_nuevos_dtype.dropna(subset=['Renta'])\n",
    "#df_limpio = df_nuevos_dtype['Renta'].dropna(inplace=True)\n",
    "\n",
    "df_acotado_x_region = df_nuevos_dtype[df_nuevos_dtype['Region'] == 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for d in df_acotado_x_region:\n",
    "    print(f\"{d} = {df_acotado_x_region[d].info()} \\n\")\n",
    "    print(f\"{d} = {df_acotado_x_region[d].describe()} \\n\")\n",
    "\n",
    "df_acotado_x_region.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc060264",
   "metadata": {},
   "source": [
    "### Vamos a dejar afuera algunas variables que no utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad_nan_renta = df_acotado_x_region['Renta'].isnull().sum()\n",
    "##cantidad_nan_region = df_acotado_x_region['Region'].isnull().sum()\n",
    "#print(\"Cantidad de NaN en Renta:\", cantidad_nan_renta)\n",
    "#print(\"Cantidad de NaN en Region:\", cantidad_nan_renta)\n",
    "\n",
    "df_acotado_parte2 = df_acotado_x_region[['FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', 'FacCN_T05', \n",
    "                                         'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10', \n",
    "                                         'FacCN_T11', 'FacCN_T12', 'FacCI_T01', 'FacCI_T02', 'FacCI_T03', \n",
    "                                         'FacCI_T04', 'FacCI_T05', 'FacCI_T06', 'FacCI_T07', 'FacCI_T08', \n",
    "                                         'FacCI_T09', 'FacCI_T10', 'FacCI_T11', 'FacCI_T12']]\n",
    "\n",
    "df_acotado_x_region = df_acotado_x_region.drop(df_acotado_x_region[['Dualidad', 'Consumo', 'Ctacte', 'Debito', 'Hipotecario',\n",
    "'TxsCN_T01', 'TxsCN_T02', 'TxsCN_T03', 'TxsCN_T04', 'TxsCN_T05', 'TxsCN_T06', 'TxsCN_T07', 'TxsCN_T08', 'TxsCN_T09', 'TxsCN_T10', 'TxsCN_T11', 'TxsCN_T12',\n",
    "'TxsCI_T01', 'TxsCI_T02', 'TxsCI_T03', 'TxsCI_T04', 'TxsCI_T05', 'TxsCI_T06', 'TxsCI_T07', 'TxsCI_T08', 'TxsCI_T09', 'TxsCI_T10', 'TxsCI_T11', 'TxsCI_T12',\n",
    "'FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', 'FacCN_T05', 'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10','FacCN_T11', 'FacCN_T12', \n",
    "'FacCI_T01', 'FacCI_T02', 'FacCI_T03','FacCI_T04', 'FacCI_T05', 'FacCI_T06', 'FacCI_T07', 'FacCI_T08', 'FacCI_T09', 'FacCI_T10', 'FacCI_T11', 'FacCI_T12',\n",
    "'UsoL1_T01','UsoL1_T02', 'UsoL1_T03', 'UsoL1_T04', 'UsoL1_T05', 'UsoL1_T06', 'UsoL1_T07', 'UsoL1_T08', 'UsoL1_T09', 'UsoL1_T10', 'UsoL1_T11', 'UsoL1_T12', \n",
    "'UsoLI_T01', 'UsoLI_T02', 'UsoLI_T03', 'UsoLI_T04', 'UsoLI_T05', 'UsoLI_T06', 'UsoLI_T07', 'UsoLI_T08', 'UsoLI_T09', 'UsoLI_T10', 'UsoLI_T11', 'UsoLI_T12', \n",
    "'PagoNac_T01', 'PagoNac_T02', 'PagoNac_T03', 'PagoNac_T04', 'PagoNac_T05', 'PagoNac_T06', \n",
    "'PagoNac_T07', 'PagoNac_T08', 'PagoNac_T09', 'PagoNac_T10', 'PagoNac_T11', 'PagoNac_T12', \n",
    "'PagoInt_T01', 'PagoInt_T02', 'PagoInt_T03', 'PagoInt_T04', 'PagoInt_T05', 'PagoInt_T06', \n",
    "'PagoInt_T07', 'PagoInt_T08', 'PagoInt_T09', 'PagoInt_T10', 'PagoInt_T11', 'PagoInt_T12', \n",
    "'FlgAct_T01', 'FlgAct_T02', 'FlgAct_T03', 'FlgAct_T04', 'FlgAct_T05', 'FlgAct_T06', 'FlgAct_T07', \n",
    "'FlgAct_T08', 'FlgAct_T09', 'FlgAct_T10', 'FlgAct_T11', 'FlgAct_T12', 'FacAN_T01', 'FacAN_T02', \n",
    "'FacAN_T03', 'FacAN_T04', 'FacAN_T05', 'FacAN_T06', 'FacAN_T07', 'FacAN_T08', 'FacAN_T09', \n",
    "'FacAN_T10', 'FacAN_T11', 'FacAN_T12', 'FacAI_T01', 'FacAI_T02', 'FacAI_T03', 'FacAI_T04', \n",
    "'FacAI_T05', 'FacAI_T06', 'FacAI_T07', 'FacAI_T08', 'FacAI_T09', 'FacAI_T10', 'FacAI_T11', \n",
    "'FacAI_T12', 'IndRev_T12','IndRev_T11','IndRev_T10','IndRev_T09','IndRev_T08',\n",
    "'IndRev_T07','IndRev_T06','IndRev_T05','IndRev_T04','IndRev_T03','IndRev_T02','IndRev_T01']], axis=1)\n",
    "\n",
    "# Concatenar df_acotado_x_region y df_acotado_parte2 horizontalmente\n",
    "df_acotado_x_region = pd.concat([df_acotado_x_region, df_acotado_parte2], axis=1)\n",
    "\n",
    "# Verificar los primeros registros para confirmar la unión\n",
    "df_acotado_x_region.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los promedios mensuales\n",
    "def calcular_promedios_mensuales(df):\n",
    "    # Calcular el promedio de las columnas FacCN_T01 a FacCN_T12\n",
    "    df['FacCN_mensual'] = df[['FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', 'FacCN_T05', \n",
    "                              'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10', \n",
    "                              'FacCN_T11', 'FacCN_T12']].mean(axis=1).round(0)\n",
    "    \n",
    "    # Calcular el promedio de las columnas FacCI_T01 a FacCI_T12\n",
    "    df['FacCI_mensual'] = df[['FacCI_T01', 'FacCI_T02', 'FacCI_T03', 'FacCI_T04', 'FacCI_T05', \n",
    "                              'FacCI_T06', 'FacCI_T07', 'FacCI_T08', 'FacCI_T09', 'FacCI_T10', \n",
    "                              'FacCI_T11', 'FacCI_T12']].mean(axis=1).round(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para calcular el gasto anual\n",
    "def calcular_gasto_anual(df):\n",
    "    # Sumar los valores de las columnas FacCN_T01 a FacCN_T12 para obtener FacCN_anual\n",
    "    df['FacCN_anual'] = df[['FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', 'FacCN_T05', \n",
    "                            'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10', \n",
    "                            'FacCN_T11', 'FacCN_T12']].sum(axis=1).round(0)\n",
    "    \n",
    "    # Sumar los valores de las columnas FacCI_T01 a FacCI_T12 para obtener FacCI_anual\n",
    "    df['FacCI_anual'] = df[['FacCI_T01', 'FacCI_T02', 'FacCI_T03', 'FacCI_T04', 'FacCI_T05', \n",
    "                            'FacCI_T06', 'FacCI_T07', 'FacCI_T08', 'FacCI_T09', 'FacCI_T10', \n",
    "                            'FacCI_T11', 'FacCI_T12']].sum(axis=1).round(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar ambas funciones para obtener el DataFrame con los promedios mensuales y gastos anuales\n",
    "df_acotado_x_region = calcular_promedios_mensuales(df_acotado_x_region)\n",
    "df_acotado_x_region = calcular_gasto_anual(df_acotado_x_region)\n",
    "\n",
    "# Eliminar las columnas FacCN_T01 a FacCN_T12 y FacCI_T01 a FacCI_T12\n",
    "df_acotado_x_region = df_acotado_x_region.drop(df_acotado_x_region[['FacCN_T01', 'FacCN_T02', 'FacCN_T03', 'FacCN_T04', 'FacCN_T05', \n",
    "                                         'FacCN_T06', 'FacCN_T07', 'FacCN_T08', 'FacCN_T09', 'FacCN_T10', \n",
    "                                         'FacCN_T11', 'FacCN_T12', 'FacCI_T01', 'FacCI_T02', 'FacCI_T03', \n",
    "                                         'FacCI_T04', 'FacCI_T05', 'FacCI_T06', 'FacCI_T07', 'FacCI_T08', \n",
    "                                         'FacCI_T09', 'FacCI_T10', 'FacCI_T11', 'FacCI_T12']], axis=1)\n",
    "\n",
    "# Mostrar las primeras 50 filas del DataFrame resultante\n",
    "df_acotado_x_region.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b4a5f",
   "metadata": {},
   "source": [
    "### Se ha detectado una inconsistencia en la gestión de los cupos de tarjetas de crédito. La columna 'CUPO_MX', destinada a registrar el límite para compras internacionales, se encuentra expresada en dólares estadounidenses (USD). Sin embargo, la facturación de las compras internacionales (FacCI) se ha realizado en pesos chilenos (CLP), lo cual genera una discrepancia en la moneda utilizada para la administración de los límites y el registro de las transacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc3b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afad24e",
   "metadata": {},
   "source": [
    "### Revisamos la cantidad de 0s en la columna Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los ceros en la columna Region\n",
    "cantidad_ceros_region = (df_acotado_x_region['Region'] == 0).sum()\n",
    "\n",
    "if cantidad_ceros_region > 0:\n",
    "    print(f\"Hay {cantidad_ceros_region} valores 0 en la columna Region.\")\n",
    "else:\n",
    "    print(\"No hay valores 0 en la columna Region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90175ccf",
   "metadata": {},
   "source": [
    "### Hacemos una pequeña estadística de los datos por cada columna, para tener mejor idea de los datos con los que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78cb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_acotado_x_region:\n",
    "  print(f\"{i} = {df_acotado_x_region[i].describe()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfaa81",
   "metadata": {},
   "source": [
    "### Revisamos los valor únicos que tiene cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_acotado_x_region:\n",
    "    print(f\"{i} = {df_acotado_x_region[i].unique()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca702124",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_acotado_x_region.columns:\n",
    "    try:\n",
    "        unique_values = df_acotado_x_region[i].unique()\n",
    "        print(f\"{i} = {unique_values} \\n\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error en la columna '{i}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5a048",
   "metadata": {},
   "source": [
    "## Identificación de Valores Faltantes\n",
    "### En este paso, identificamos las columnas que contienen valores faltantes dentro del dataset. Esto es importante ya que los algoritmos de Machine Learning generalmente no funcionan bien con datos incompletos. Visualizamos estos valores faltantes usando un mapa de calor para identificar qué columnas requieren atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos cuántos valores faltantes hay por columna\n",
    "# Iterar por cada columna del DataFrame\n",
    "for columna in df_acotado_x_region.columns:\n",
    "    total_faltantes = df_acotado_x_region[columna].isna().sum()\n",
    "    porcentaje_faltantes = (df_acotado_x_region[columna].isna().mean() * 100)\n",
    "    tipo_dato = df_acotado_x_region[columna].dtype\n",
    "\n",
    "    # Mostrar la información de la columna actual\n",
    "    print(f\"Columna: {columna}\")\n",
    "    print(f\"  - Total Faltantes: {total_faltantes}\")\n",
    "    print(f\"  - Porcentaje Faltantes: {porcentaje_faltantes:.2f}%\")\n",
    "    print(f\"  - Tipo de Dato: {tipo_dato}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109c3f5",
   "metadata": {},
   "source": [
    "### Eliminamos la columna Región."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acotado_x_region = df_acotado_x_region.drop('Region', axis=1)\n",
    "#df_acotado_x_region = df_acotado_x_region.drop('Id', axis=1)\n",
    "# df_acotado_x_region.to_csv('/Users/herna/Desktop/df_acotado_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ac9db",
   "metadata": {},
   "source": [
    "## Detección y Manejo de Outliers\n",
    "### Los outliers pueden influir negativamente en los resultados del análisis y el modelado. Aquí, utilizamos el método del rango intercuartil (IQR) para identificar y eliminar los outliers en las variables numéricas, asegurando que no distorsionen las relaciones entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que df_acotado_x_region es el dataframe original\n",
    "df_acotado_x_region_v2 = df_acotado_x_region.copy()\n",
    "\n",
    "# Listado de columnas numéricas relevantes para detectar outliers\n",
    "num_columnas = ['Edad', 'Renta', 'Antiguedad', 'CUPO_L1', 'CUPO_MX']\n",
    "\n",
    "# Función para detectar outliers con el método IQR en cualquier columna numérica\n",
    "def detectar_outliers(df, columna):\n",
    "    Q1 = df[columna].quantile(0.25)\n",
    "    Q3 = df[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # Definir los límites para identificar los outliers\n",
    "    outliers = df[(df[columna] < (Q1 - 1.5 * IQR)) | (df[columna] > (Q3 + 1.5 * IQR))]\n",
    "    return outliers\n",
    "\n",
    "# Visualización de outliers con gráficos de caja para una columna\n",
    "def visualizar_outliers(df, columna):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sb.boxplot(x=df[columna])\n",
    "    plt.title(f'Boxplot de {columna}')\n",
    "    plt.show()\n",
    "\n",
    "# Detección, visualización y eliminación de outliers para todas las columnas numéricas\n",
    "all_outliers = pd.DataFrame()  # Para acumular todos los outliers detectados\n",
    "\n",
    "for columnas in num_columnas:\n",
    "    # Detectar outliers en la columna actual\n",
    "    outliers_col = detectar_outliers(df_acotado_x_region_v2, columnas)\n",
    "    print(f\"Outliers detectados en '{columnas}': {outliers_col.shape[0]}\")\n",
    "    \n",
    "    # Visualizar los outliers en la columna actual\n",
    "    visualizar_outliers(df_acotado_x_region_v2, columnas)\n",
    "    \n",
    "    # Acumular los outliers detectados\n",
    "    all_outliers = pd.concat([all_outliers, outliers_col])\n",
    "\n",
    "# Eliminar los outliers detectados de todas las columnas\n",
    "df_acotado_x_region_v2 = df_acotado_x_region_v2[~df_acotado_x_region_v2.index.isin(all_outliers.index)]\n",
    "\n",
    "# Mostrar el número de filas después de eliminar los outliers\n",
    "print(f\"Filas después de eliminar outliers: {df_acotado_x_region_v2.shape[0]}\")\n",
    "\n",
    "# Ver los primeros 25 registros después de eliminar outliers\n",
    "df_acotado_x_region_v2.head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dae25",
   "metadata": {},
   "source": [
    "### Imputamos con el modelo Regresión Lineal, porque es más confiable que la imputación con la media o mediana, y se basa en las relaciones entre múltiples variables del dataset, haciendo que las imputaciones sean más precisas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar los datos con y sin valores faltantes en la columna 'Renta'\n",
    "con_renta = df_acotado_x_region_v2[df_acotado_x_region_v2['Renta'].notnull()]\n",
    "sin_renta = df_acotado_x_region_v2[df_acotado_x_region_v2['Renta'].isnull()]\n",
    "\n",
    "# Definir las columnas predictoras (excluimos 'Renta', 'Unnamed: 0', 'target')\n",
    "predictores = ['Edad', 'Sexo', 'Cuentas', 'Antiguedad']\n",
    "\n",
    "# Separar las variables predictoras y la variable objetivo ('Renta')\n",
    "X = con_renta[predictores]\n",
    "y = con_renta['Renta']\n",
    "\n",
    "# Preprocesamiento: OneHotEncoding para la columna 'Sexo' (es categórica)\n",
    "preprocesador = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(drop='first'), ['Sexo'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Crear un pipeline que combine preprocesamiento y modelo\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocesador),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entrenar el modelo de regresión\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Ahora usamos este modelo para predecir los valores faltantes de 'Renta'\n",
    "X_faltantes = sin_renta[predictores]\n",
    "\n",
    "# Predecir los valores de Renta faltantes\n",
    "renta_pred = pipeline.predict(X_faltantes)\n",
    "\n",
    "# Imputar los valores predichos en el dataframe original\n",
    "df_acotado_x_region_v2.loc[df_acotado_x_region_v2['Renta'].isnull(), 'Renta'] = renta_pred\n",
    "\n",
    "# Mostrar el dataframe con la columna 'Renta' imputada\n",
    "df_acotado_x_region_v2.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5999fd1",
   "metadata": {},
   "source": [
    "### Vamos a borrar y verificar los nulos en las columnas del Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_checkear = ['Edad', 'Renta', 'Sexo', 'TC', 'Cuentas', 'Antiguedad', 'CUPO_L1', 'CUPO_MX', 'target']\n",
    "\n",
    "# Mostrar la cantidad de filas antes de la limpieza\n",
    "filas_iniciales = df_acotado_x_region_v2.shape[0]\n",
    "\n",
    "# Eliminar las filas que contengan valores nulos en las columnas seleccionadas\n",
    "df_acotado_x_region_v2 = df_acotado_x_region_v2.dropna(subset=columnas_a_checkear)\n",
    "\n",
    "# Mostrar la cantidad de filas después de la limpieza\n",
    "filas_limpias = df_acotado_x_region_v2.shape[0]\n",
    "\n",
    "print(f\"Filas iniciales: {filas_iniciales}\")\n",
    "print(f\"Filas después de eliminar valores nulos en las columnas seleccionadas: {filas_limpias}\")\n",
    "\n",
    "porcentaje_reduccion = ((filas_iniciales / filas_limpias) * 100) - 100\n",
    "porcentaje_reduccion_redondeado = round(porcentaje_reduccion, 1)\n",
    "\n",
    "print(f\"Porcentaje de reducción de datos: {porcentaje_reduccion_redondeado}%\")\n",
    "\n",
    "df_acotado_x_region_v2.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0bfcd",
   "metadata": {},
   "source": [
    "## Codificación de Variables Categóricas\n",
    "### Para poder utilizar las variables categóricas en modelos de Machine Learning, es necesario transformarlas en representaciones numéricas. Para esto, utilizamos **Label Encoding** para variables categóricas con pocos valores (como 'Sexo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los valores únicos de la columna 'Region'\n",
    "print(df_acotado_x_region_v2['Sexo'].unique())\n",
    "\n",
    "# Ver la cantidad de valores únicos\n",
    "print(f\"Cantidad de valores únicos: {df_acotado_x_region_v2['Sexo'].nunique()}\")\n",
    "\n",
    "# Revisar si hay valores nulos en la columna 'Region'\n",
    "print(f\"Cantidad de valores nulos: {df_acotado_x_region_v2['Sexo'].isnull().sum()}\")\n",
    "\n",
    "# Hacer un conteo de la cantidad de veces que aparece cada valor único en la columna 'Region'\n",
    "print(df_acotado_x_region_v2['Sexo'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92277064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos Label Encoding para 'Sexo', ya que solo tiene unos pocos valores\n",
    "label_encoder = LabelEncoder()\n",
    "df_acotado_x_region_v2['Sexo'] = label_encoder.fit_transform(df_acotado_x_region_v2['Sexo'])\n",
    "df_preparado = df_acotado_x_region_v2\n",
    "\n",
    "df_preparado.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fad1b",
   "metadata": {},
   "source": [
    "### Ahora salió una incógnita, ya que utilizamos el label encoding para las columnas, necesitamos primeramente identificar qué valores entre 0 y 1 serán los sexos \"Masculino\" y \"Femenino\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_interes = ['Sexo']\n",
    "\n",
    "conteo_ceros = df_preparado[columnas_interes].apply(lambda x: (x == True).sum())\n",
    "conteo_unos = df_preparado[columnas_interes].apply(lambda x: (x == False).sum())\n",
    "conteo_dos = df_preparado[columnas_interes].apply(lambda x: (x.isna()).sum())\n",
    "\n",
    "resultados_conteo = pd.DataFrame({\n",
    "    '0': conteo_ceros, # Femenino\n",
    "    '1': conteo_unos, # Masculino\n",
    "    '2': conteo_dos # EN el caso de haber quedado valores nulos.\n",
    "\n",
    "})\n",
    "\n",
    "print(resultados_conteo)\n",
    "print(\"El valor con mayor cantidad serán los hombres ya que logramos apreciar anteriormente que hay más clientes de sexo masculino\")\n",
    "resultados_conteo.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preparado.apply(lambda col: print(f\"\\nDescripción de la columna '{col.name}':\\n\", col.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a98f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas que no contengan valores negativos\n",
    "df_filtrado = df_preparado[(df_preparado >= 0).all(axis=1)]\n",
    "\n",
    "\n",
    "# Mostrar las primeras filas del dataframe filtrado\n",
    "df_filtrado.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad83026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado.apply(lambda col: print(f\"\\nDescripción de la columna '{col.name}':\\n\", col.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos cuántos valores faltantes hay por columna\n",
    "# Iterar por cada columna del DataFrame\n",
    "for columna in df_filtrado.columns:\n",
    "    total_faltantes = df_filtrado[columna].isna().sum()\n",
    "    porcentaje_faltantes = (df_filtrado[columna].isna().mean() * 100)\n",
    "    tipo_dato = df_filtrado[columna].dtype\n",
    "\n",
    "    # Mostrar la información de la columna actual\n",
    "    print(f\"Columna: {columna}\")\n",
    "    print(f\"  - Total Faltantes: {total_faltantes}\")\n",
    "    print(f\"  - Porcentaje Faltantes: {porcentaje_faltantes:.2f}%\")\n",
    "    print(f\"  - Tipo de Dato: {tipo_dato}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "df_filtrado.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ac6f8",
   "metadata": {},
   "source": [
    "### Revisaremos con gráficas el dataframe para guiarnos en los siguientes pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las columnas que queremos analizar en detalle\n",
    "columnas_para_filtrar = ['Edad', 'Region', 'Antiguedad']\n",
    "\n",
    "# Iteramos sobre cada columna del DataFrame, excluyendo un gran conjunto de columnas\n",
    "for columna in df_filtrado.drop(df_filtrado[['target']], axis=1):\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    sb.histplot(df_filtrado[columna], bins=200)\n",
    "    plt.title(columna)\n",
    "    plt.xlabel(columna)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "    # Si la columna está en la lista de columnas a filtrar, ajustamos los límites de los ejes x\n",
    "    if columna in columnas_para_filtrar:\n",
    "        valor_max = df_filtrado[columna].max()\n",
    "        valor_min = df_filtrado[columna].min()\n",
    "        plt.xticks(np.arange(valor_min, valor_max + 1, step=1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851f107",
   "metadata": {},
   "source": [
    "## Seleccionaremos características, y convertiremos a enteros las que sean necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c076264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas faltantes para conversión\n",
    "conversion_col_faltantes = ['Renta', 'Sexo', 'CUPO_L1', 'CUPO_MX']\n",
    "# Aplicar la transformación a cada columna\n",
    "for columna in conversion_col_faltantes:\n",
    "    # Redondear y convertir a int64\n",
    "    df_filtrado[columna] = np.round(df_filtrado[columna]).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado_v2 = df_filtrado.copy()\n",
    "\n",
    "X = df_filtrado_v2.drop(columns=['Renta'])\n",
    "y = df_filtrado_v2['Renta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc6511",
   "metadata": {},
   "source": [
    "### Mostramos las características más relevantes para la predicción de la variable objetivo \"y\" en un conjunto de datos \"X\" utilizando una prueba estadística de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f334fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_cat = SelectKBest(score_func=f_regression, k=12)\n",
    "fit = mejores_cat.fit(X, y)\n",
    "\n",
    "puntaje_df = pd.DataFrame(fit.scores_)\n",
    "columnas_df = pd.DataFrame(X.columns)\n",
    "\n",
    "puntajes_cat = pd.concat([columnas_df, puntaje_df], axis=1)\n",
    "puntajes_cat.columns = ['Descripción', 'Resultado']\n",
    "\n",
    "puntajes_cat.nlargest(12, 'Resultado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ab75b",
   "metadata": {},
   "source": [
    "### Obtenemos importancia de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ExtraTreesRegressor()\n",
    "modelo.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf4d95",
   "metadata": {},
   "source": [
    "Mostramos gráficamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_cat = pd.Series(modelo.feature_importances_, index=X.columns)\n",
    "importancia_cat.nlargest(14).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_normalizado.head(20)\n",
    "df_filtrado_v2.head(25)\n",
    "for i in df_filtrado_v2:\n",
    "    print(f\"{i} = {df_filtrado_v2[i].unique()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ed1bd",
   "metadata": {},
   "source": [
    "Procederemos a tomar la variable contínua Edad, creando una nueva variable de tipo categórica ('Rango_Edad) con nuevas 'Features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [18, 30, 60, 100]\n",
    "labels = ['Joven' , 'Adulto', 'Adulto Mayor']\n",
    "\n",
    "df_filtrado_v2['Rango_Edad'] = pd.cut(df_filtrado_v2['Edad'], bins=bins, labels=labels)\n",
    "\n",
    "orden_rangos = {'Joven': 0, 'Adulto': 1, 'Adulto Mayor': 2}\n",
    "\n",
    "df_filtrado_v2['Rango_Edad'] = df_filtrado_v2['Rango_Edad'].map(orden_rangos)\n",
    "\n",
    "df_filtrado_v2[['Edad', 'Rango_Edad']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_rangos_edad(df_filtrado_v2 ):\n",
    "    columna = 'Rango_Edad'\n",
    "    if columna in df_filtrado_v2 .columns:\n",
    "        conteos = df_filtrado_v2 [columna].value_counts()\n",
    "        rangos_edad = [0, 1, 2]\n",
    "        conteos_rangos = {rango: conteos.get(rango, 0) for rango in rangos_edad}\n",
    "        return conteos_rangos\n",
    "    else:\n",
    "        raise ValueError(f\"La columna '{columna}' no se encuentra en el DataFrame\")\n",
    "\n",
    "conteos_rangos_edad = contar_rangos_edad(df_filtrado_v2)\n",
    "print(conteos_rangos_edad)\n",
    "\n",
    "df_filtrado_v2.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado_v2.drop(columns=['Unnamed: 0'], errors='ignore', inplace=True)\n",
    "df_filtrado_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado_v2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eced2e0",
   "metadata": {},
   "source": [
    "Pasamos la variable 'CUPO_MX', donde sus valores están reflejados en dólares estadounidenses, a pesos chilenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba274b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_cambio = 816.36\n",
    "cupo_mean = df_filtrado_v2['CUPO_MX'] * valor_cambio\n",
    "\n",
    "df_filtrado_v2['CUPO_MX_CLP'] = cupo_mean.round(0).astype('int64')\n",
    "\n",
    "df_filtrado_v2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado_v2 = df_filtrado_v2.drop(columns=['target', 'Id'])\n",
    "df_filtrado_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado_v2['FacCN_anual'] = df_filtrado_v2['FacCN_anual'].astype('int64')\n",
    "df_filtrado_v2['FacCN_mensual'] = df_filtrado_v2['FacCN_mensual'].astype('int64')\n",
    "df_filtrado_v2['FacCI_anual'] = df_filtrado_v2['FacCI_anual'].astype('int64')\n",
    "df_filtrado_v2['FacCI_mensual'] = df_filtrado_v2['FacCI_mensual'].astype('int64')\n",
    "df_filtrado_v2['Rango_Edad'] = df_filtrado_v2['Rango_Edad'].astype('int64')\n",
    "df_filtrado_v2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50fe12e",
   "metadata": {},
   "source": [
    "### Normalizaremos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia del DataFrame original\n",
    "df_normalizado = df_filtrado_v2.copy()\n",
    "\n",
    "# Separar las columnas que van a usarse con StandardScaler\n",
    "col_minmax = ['Edad', 'Antiguedad', 'CUPO_L1', 'CUPO_MX', 'Cuentas', 'Sexo', 'TC']\n",
    "\n",
    "# Inicializar los escaladores\n",
    "#scaler_standard = MinMaxScalerScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Aplicar StandardScaler\n",
    "#df_normalizado[col_standard] = scaler_standard.fit_transform(df_normalizado[col_standard])\n",
    "\n",
    "# Aplicar MinMaxScaler\n",
    "df_normalizado[col_minmax] = scaler_minmax.fit_transform(df_normalizado[col_minmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526e8ba",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25431c98",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c587fb",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
